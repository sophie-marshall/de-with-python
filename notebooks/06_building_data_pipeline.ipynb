{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Data Pipeline \n",
    "\n",
    "**Goal:** Build an Airflow pipeline that takes data from the See Click Fix API and upserts it into elasticsearch for eventual use in Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Testing\n",
    "\n",
    "Tasks to Complete: \n",
    "- Query the API (make sure to gather historical and paginated data)\n",
    "- Upsert to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "def query_scf_api(\n",
    "    place_url: str, per_page: str = \"100\", get_historical_data: bool = False\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Gather data from See Click Fix API for a specified location\n",
    "\n",
    "    Args:\n",
    "        - place_url (str): Parameter specifying the city/location you want information about\n",
    "        - per_page (str): How many results shoudl the API return\n",
    "        - get_historical_data (bool): Should the API look backwards and get past data?\n",
    "\n",
    "    Returns:\n",
    "        - issues (list): List of issues returned by the SCF API\n",
    "    \"\"\"\n",
    "    # set request params\n",
    "    params = {\"place_url\": place_url, \"per_page\": per_page}\n",
    "    # if we want historical data, add a new param to look backwards\n",
    "    if get_historical_data:\n",
    "        params[\"status\"] = \"Archived\"\n",
    "    # send request\n",
    "    res = requests.get(url=\"https://seeclickfix.com/api/v2/issues\", params=params)\n",
    "    # grab data if request is successfull\n",
    "    if res.status_code == 200:\n",
    "        data = res.json()\n",
    "        issues = data[\"issues\"]\n",
    "        # get total pages and current page values\n",
    "        curr = data[\"metadata\"][\"pagination\"][\"page\"]\n",
    "        total = data[\"metadata\"][\"pagination\"][\"pages\"]\n",
    "        print(f\"Available Pages: {total}\\n\")\n",
    "        # while there are still pages left continue gathering data\n",
    "        while curr <= total:\n",
    "            print(f\"Current Page: {curr}\")\n",
    "            # introduce sleep timer to let API breathe\n",
    "            if curr % 5 == 0:\n",
    "                time.sleep(10)\n",
    "            if data[\"metadata\"][\"pagination\"][\"next_page_url\"]:\n",
    "                res = requests.get(\n",
    "                    url=data[\"metadata\"][\"pagination\"][\"next_page_url\"], params=params\n",
    "                )\n",
    "                data = res.json()\n",
    "                issues.extend(data[\"issues\"])\n",
    "                curr = data[\"metadata\"][\"pagination\"][\"page\"]\n",
    "            else:\n",
    "                break\n",
    "        return {\"body\": issues, \"statusCode\": 200}\n",
    "    else:\n",
    "        return {\n",
    "            \"body\": None,\n",
    "            \"statusCode\": res.status_code,\n",
    "            \"message\": \"Error getting API data\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Pages: 5\n",
      "\n",
      "Current Page: 1\n",
      "Current Page: 2\n",
      "Current Page: 3\n",
      "Current Page: 4\n",
      "Current Page: 5\n"
     ]
    }
   ],
   "source": [
    "# get non historical data\n",
    "curr_res = query_scf_api(place_url=\"downtown_district-of-columbia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Pages: 43\n",
      "\n",
      "Current Page: 1\n",
      "Current Page: 2\n",
      "Current Page: 3\n",
      "Current Page: 4\n",
      "Current Page: 5\n",
      "Current Page: 6\n",
      "Current Page: 7\n",
      "Current Page: 8\n",
      "Current Page: 9\n",
      "Current Page: 10\n",
      "Current Page: 11\n",
      "Current Page: 12\n",
      "Current Page: 13\n",
      "Current Page: 14\n",
      "Current Page: 15\n",
      "Current Page: 16\n",
      "Current Page: 17\n",
      "Current Page: 18\n",
      "Current Page: 19\n",
      "Current Page: 20\n",
      "Current Page: 21\n",
      "Current Page: 22\n",
      "Current Page: 23\n",
      "Current Page: 24\n",
      "Current Page: 25\n",
      "Current Page: 26\n",
      "Current Page: 27\n",
      "Current Page: 28\n",
      "Current Page: 29\n",
      "Current Page: 30\n",
      "Current Page: 31\n",
      "Current Page: 32\n",
      "Current Page: 33\n",
      "Current Page: 34\n",
      "Current Page: 35\n",
      "Current Page: 36\n",
      "Current Page: 37\n",
      "Current Page: 38\n",
      "Current Page: 39\n",
      "Current Page: 40\n",
      "Current Page: 41\n",
      "Current Page: 42\n",
      "Current Page: 43\n"
     ]
    }
   ],
   "source": [
    "# get historical data\n",
    "hist_res = query_scf_api(place_url=\"downtown_district-of-columbia\", get_historical_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "def upsert_to_elasticsearch(\n",
    "    host: str, ssl_assert_fingerprint: str, username: str, password: str, index: str, data: list\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Upsert collected data to Elasticsearch index\n",
    "\n",
    "    Args:\n",
    "        - host (str): Elasticsearch host address\n",
    "        - ssl_assert_fingerprint (str): Fingerprint for Elasticsearch services\n",
    "        - username (str): Elasticsearch username\n",
    "        - password (str): Elasticsearch password\n",
    "        - index (str): Name of Elasticsearch index to upsert data to\n",
    "        - data (list): List of JSON entries to upsert to Elasticsearch\n",
    "\n",
    "    Returns:\n",
    "        - JSON response indicating status of operation and associated messaging\n",
    "    \"\"\"\n",
    "    # establish elasticsearch connection\n",
    "    es = Elasticsearch(\n",
    "        {host},\n",
    "        ssl_assert_fingerprint=ssl_assert_fingerprint,\n",
    "        http_auth=(username, password),\n",
    "    )\n",
    "    # upsert provided data to elasticsearch\n",
    "    try:\n",
    "        for item in data:\n",
    "            res = es.index(index=index, id=item[\"id\"], body=item)\n",
    "        return {\"statusCode\": 200, \"message\": \"success!\"}\n",
    "    except Exception as e:\n",
    "        return {\"statusCode\": 400, \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# test\n",
    "es_response = upsert_to_elasticsearch(\n",
    "    host=\"https://localhost:9200\", \n",
    "    ssl_assert_fingerprint=os.getenv(\"ES_FINGERPRINT\"), \n",
    "    username=\"elastic\", \n",
    "    password=os.getenv(\"ES_PASSWORD\"), \n",
    "    index=\"scf-downtown-district-of-columbia\", \n",
    "    data = res[\"body\"]\n",
    ")\n",
    "\n",
    "print(es_response[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(current_data: list, historical_data: list) -> list:\n",
    "    \"\"\"\n",
    "    Combine current data with historical data \n",
    "\n",
    "    Args: \n",
    "        - current_data (list): List of items returned by SCF API for current events\n",
    "        - historical_data (list): List of items returned by SCF API for historical events\n",
    "    \n",
    "    Returns: \n",
    "        - JSON style response with combined results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        combined_results = current_data + historical_data\n",
    "        return {\"body\": combined_results, \"statusCode\": 200, \"message\": \"success!\"}\n",
    "    except Exception as e:\n",
    "        return {\"body\": None, \"statusCode\": 400, \"message\": e}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from elasticsearch import Elasticsearch\n",
    "import requests\n",
    "\n",
    "\n",
    "## ----- HELPERS ----- ##\n",
    "def query_scf_api(\n",
    "    place_url: str, per_page: str = \"100\", get_historical_data: bool = False\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Gather data from See Click Fix API for a specified location\n",
    "\n",
    "    Args:\n",
    "        - place_url (str): Parameter specifying the city/location you want information about\n",
    "        - per_page (str): How many results shoudl the API return\n",
    "        - get_historical_data (bool): Should the API look backwards and get past data?\n",
    "\n",
    "    Returns:\n",
    "        - issues (list): List of issues returned by the SCF API\n",
    "    \"\"\"\n",
    "    # set request params\n",
    "    params = {\"place_url\": place_url, \"per_page\": per_page}\n",
    "    # if we wnat historical data, add a new param to look backwards\n",
    "    if get_historical_data:\n",
    "        params[\"status\"] = \"Archived\"\n",
    "    # send request\n",
    "    res = requests.get(url=\"https://seeclickfix.com/api/v2/issues\", params=params)\n",
    "    # grab data if request is successfull\n",
    "    if res.status_code == 200:\n",
    "        data = res.json()\n",
    "        issues = data[\"issues\"]\n",
    "        # get total pages and current page values\n",
    "        curr = data[\"metadata\"][\"pagination\"][\"page\"]\n",
    "        total = data[\"metadata\"][\"pagination\"][\"pages\"]\n",
    "        print(f\"Available Pages: {total}\\n\")\n",
    "        # while there are still pages left continue gathering data\n",
    "        while curr <= total:\n",
    "            print(f\"Current Page: {curr}\")\n",
    "            if data[\"metadata\"][\"pagination\"][\"next_page_url\"]:\n",
    "                res = requests.get(\n",
    "                    url=data[\"metadata\"][\"pagination\"][\"next_page_url\"], params=params\n",
    "                )\n",
    "                data = res.json()\n",
    "                issues.extend(data[\"issues\"])\n",
    "                curr = data[\"metadata\"][\"pagination\"][\"page\"]\n",
    "            else:\n",
    "                break\n",
    "        return {\"body\": issues, \"statusCode\": 200}\n",
    "    else:\n",
    "        return {\n",
    "            \"body\": None,\n",
    "            \"statusCode\": res.status_code,\n",
    "            \"message\": \"Error getting API data\",\n",
    "        }\n",
    "\n",
    "\n",
    "def combine_results(current_data: list, historical_data: list) -> list:\n",
    "    \"\"\"\n",
    "    Combine current data with historical data\n",
    "\n",
    "    Args:\n",
    "        - current_data (list): List of items returned by SCF API for current events\n",
    "        - historical_data (list): List of items returned by SCF API for historical events\n",
    "\n",
    "    Returns:\n",
    "        - JSON style response with combined results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        combined_results = current_data + historical_data\n",
    "        return {\"body\": combined_results, \"statusCode\": 200, \"message\": \"success!\"}\n",
    "    except Exception as e:\n",
    "        return {\"body\": None, \"statusCode\": 400, \"message\": e}\n",
    "\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "def upsert_to_elasticsearch(\n",
    "    host: str,\n",
    "    ssl_assert_fingerprint: str,\n",
    "    username: str,\n",
    "    password: str,\n",
    "    index: str,\n",
    "    data: list,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Upsert collected data to Elasticsearch index\n",
    "\n",
    "    Args:\n",
    "        - host (str): Elasticsearch host address\n",
    "        - ssl_assert_fingerprint (str): Fingerprint for Elasticsearch services\n",
    "        - username (str): Elasticsearch username\n",
    "        - password (str): Elasticsearch password\n",
    "        - index (str): Name of Elasticsearch index to upsert data to\n",
    "        - data (list): List of JSON entries to upsert to Elasticsearch\n",
    "\n",
    "    Returns:\n",
    "        - JSON response indicating status of operation and associated messaging\n",
    "    \"\"\"\n",
    "    # establish elasticsearch connection\n",
    "    es = Elasticsearch(\n",
    "        {host},\n",
    "        ssl_assert_fingerprint=ssl_assert_fingerprint,\n",
    "        http_auth=(username, password),\n",
    "    )\n",
    "    # upsert provided data to elasticsearch\n",
    "    try:\n",
    "        for item in data:\n",
    "            res = es.index(index=index, id=item[\"id\"], body=item)\n",
    "        return {\"statusCode\": 200, \"message\": \"success!\"}\n",
    "    except Exception as e:\n",
    "        return {\"statusCode\": 400, \"message\": str(e)}\n",
    "\n",
    "\n",
    "## ----- DAG ----- ##\n",
    "default_args = {\n",
    "    \"owner\": \"srmarshall\",\n",
    "    \"start_date\": datetime.datetime(2024, 7, 17),\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": datetime.timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"upsert_to_scf_index\", default_args=default_args, schedule=\"@daily\"\n",
    ") as dag:\n",
    "\n",
    "    getCurrentData = PythonOperator(\n",
    "        task_id=\"GetCurrentData\",\n",
    "        python_callable=query_scf_api,\n",
    "        op_kwargs={\"place_url\": \"downtown_district-of-columbia\"},\n",
    "    )\n",
    "\n",
    "    getHistoricalData = PythonOperator(\n",
    "        task_id=\"GetHistoricalData\",\n",
    "        python_callable=query_scf_api,\n",
    "        op_kwargs={\n",
    "            \"place_url\": \"downtown_district-of-columbia\",\n",
    "            \"get_historical_data\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    combineResults = PythonOperator(\n",
    "        task_id=\"CombineResults\",\n",
    "        python_callable=combine_results,\n",
    "        op_kwargs={\n",
    "            \"current_data\": \"{{ ti.xcom_pull(task_ids='GetCurrentData')['body'] }}\",\n",
    "            \"historical_data\": \"{{ ti.xcom_pull(task_ids='GetHistoricalData')['body'] }}\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    upsertToElasticsearch = PythonOperator(\n",
    "        task_id=\"UpsertToElasticsearch\",\n",
    "        python_callable=upsert_to_elasticsearch,\n",
    "        op_kwargs={\n",
    "            \"host\": \"https://localhost:9200\",\n",
    "            \"ssl_assert_fingerprint\": os.getenv(\"ES_FINGERPRINT\"),\n",
    "            \"username\": \"elastic\",\n",
    "            \"password\": os.getenv(\"ES_PASSWORD\"),\n",
    "            \"index\": \"scf-downtown-district-of-columbia\",\n",
    "            \"data\": \"{{ ti.xcom_pull(task_ids='CombineResults')['body'] }}\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # set dependencies\n",
    "    [getCurrentData, getHistoricalData] >> combineResults\n",
    "    combineResults >> upsertToElasticsearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
